{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da68860d",
   "metadata": {},
   "source": [
    "## Image Extraction Helper Function\n",
    "\n",
    "This helper function checks if an image URL is valid (not an ad) and is related to scholarships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b68c76ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_image(url):\n",
    "    \"\"\"Check if image URL is valid scholarship image and not an advertisement.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The image URL to check\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the image is valid scholarship image, False otherwise\n",
    "    \"\"\"\n",
    "    # Skip ad-related images\n",
    "    ad_domains = ['ezodn.com', 'ezcdn.com', 'doubleclick.net', 'google.com']\n",
    "    if any(domain in url.lower() for domain in ad_domains):\n",
    "        return False\n",
    "    \n",
    "    # Look for scholarship-related images\n",
    "    scholarship_indicators = ['wp-content/uploads', 'scholarship', 'study', 'university', 'education']\n",
    "    return any(indicator in url.lower() for indicator in scholarship_indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccfe534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_scholarships(url, region_name):\n",
    "    \"\"\"Scrape scholarships for a specific region.\"\"\"\n",
    "    print(f\"\\n=== Scraping {region_name} scholarships ===\")\n",
    "    \n",
    "    # Setup CSV\n",
    "    csv_filename = f\"scholarships-{region_name.lower().replace(' ', '-')}.csv\"\n",
    "    csv_file = open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\")\n",
    "    csv_writer = csv.DictWriter(csv_file, fieldnames=[\n",
    "        \"Title\", \"Description\", \"Link\", \"Official Link\", \"Image\", \"Deadline\", \"Eligibility\",\n",
    "        \"Host Country\", \"Host University\", \"Program Duration\", \"Degree Offered\", \"Region\", \"Post_at\"\n",
    "    ])\n",
    "    csv_writer.writeheader()\n",
    "    \n",
    "    try:\n",
    "        # Load page\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Get links\n",
    "        post_elements = driver.find_elements(By.CSS_SELECTOR, \"h2.entry-title a\")\n",
    "        post_links = [elem.get_attribute(\"href\") for elem in post_elements]\n",
    "        print(f\"Found {len(post_links)} posts in {region_name}.\")\n",
    "        \n",
    "        # Loop through each post\n",
    "        for index, link in enumerate(post_links):\n",
    "            try:\n",
    "                driver.get(link)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                \n",
    "                # Title\n",
    "                title_element = soup.find(\"h1\", class_=\"entry-title\")\n",
    "                if not title_element:\n",
    "                    print(f\"‚ö†Ô∏è No title found for post {index+1}. Skipping...\")\n",
    "                    continue\n",
    "                    \n",
    "                title = title_element.text.strip()\n",
    "                \n",
    "                # Content area\n",
    "                content_div = soup.find(\"div\", class_=\"entry-content\")\n",
    "                if not content_div:\n",
    "                    print(f\"‚ö†Ô∏è No content found for post: {title}. Skipping...\")\n",
    "                    continue\n",
    "                \n",
    "                # Official Link\n",
    "                official_link = \"\"\n",
    "                for a in content_div.find_all(\"a\"):\n",
    "                    if not a.text:\n",
    "                        continue\n",
    "                    if \"official\" in a.text.lower() or \"apply\" in a.text.lower():\n",
    "                        official_link = a.get(\"href\")\n",
    "                        break\n",
    "                \n",
    "                # Extract sections\n",
    "                deadline = extract_section(content_div, [\"deadline\", \"last date\", \"closing date\"])\n",
    "                description = extract_description(content_div)\n",
    "                eligibility = extract_section(content_div, [\"eligibility\", \"who can apply\", \"eligible\"])\n",
    "                host_country = extract_section(content_div, [\"host country\", \"study in\", \"country\"])\n",
    "                host_university = extract_section(content_div, [\"host university\", \"offered by\", \"university\"])\n",
    "                program_duration = extract_section(content_div, [\"program duration\", \"duration\"])\n",
    "                degree_offered = extract_section(content_div, [\"degree\", \"degree offered\", \"field of study\", \"what you will study\"])\n",
    "                \n",
    "                # Extract post date\n",
    "                post_at = \"\"\n",
    "                post_at_element = soup.select_one(\".entry-date.published\")\n",
    "                if post_at_element:\n",
    "                    post_at = post_at_element.get_text(strip=True)\n",
    "                    \n",
    "                # Featured Image\n",
    "                image_url = \"\"\n",
    "                \n",
    "                # First try to get the featured image from article header\n",
    "                article = soup.find('article')\n",
    "                if article:\n",
    "                    # Look for featured image div first\n",
    "                    featured_div = article.find('div', class_='post-thumbnail') or article.find('div', class_='featured-image')\n",
    "                    if featured_div:\n",
    "                        img_tag = featured_div.find('img')\n",
    "                        if img_tag and img_tag.get('src'):\n",
    "                            raw_src = img_tag['src'].split('?')[0]\n",
    "                            full_url = urljoin(link, raw_src)\n",
    "                            if is_valid_image(full_url):\n",
    "                                image_url = full_url\n",
    "                    \n",
    "                    # If no featured image found, try other images in the article\n",
    "                    if not image_url:\n",
    "                        for img in article.find_all('img'):\n",
    "                            if img.get('src'):\n",
    "                                raw_src = img['src'].split('?')[0]\n",
    "                                full_url = urljoin(link, raw_src)\n",
    "                                if is_valid_image(full_url):\n",
    "                                    image_url = full_url\n",
    "                                    break\n",
    "                \n",
    "                # If still no image found, look in content div\n",
    "                if not image_url and content_div:\n",
    "                    for img_tag in content_div.find_all('img'):\n",
    "                        if img_tag.get('src'):\n",
    "                            raw_src = img_tag['src'].split('?')[0]\n",
    "                            full_url = urljoin(link, raw_src)\n",
    "                            if is_valid_image(full_url):\n",
    "                                image_url = full_url\n",
    "                                break\n",
    "                \n",
    "                # Save\n",
    "                csv_writer.writerow({\n",
    "                    \"Title\": title,\n",
    "                    \"Description\": description,\n",
    "                    \"Link\": link,\n",
    "                    \"Official Link\": official_link,\n",
    "                    \"Image\": image_url,\n",
    "                    \"Deadline\": deadline,\n",
    "                    \"Eligibility\": eligibility,\n",
    "                    \"Host Country\": host_country,\n",
    "                    \"Host University\": host_university,\n",
    "                    \"Program Duration\": program_duration,\n",
    "                    \"Degree Offered\": degree_offered,\n",
    "                    \"Region\": region_name,\n",
    "                    \"Post_at\": post_at\n",
    "                })\n",
    "                \n",
    "                print(f\"‚úÖ {region_name} - Saved {index+1}/{len(post_links)}: {title[:50]}...\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error processing post {index+1} in {region_name} region: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return csv_filename\n",
    "    \n",
    "    finally:\n",
    "        csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b35ab73",
   "metadata": {},
   "source": [
    "# Scholarship Data Scraping\n",
    "\n",
    "This notebook scrapes scholarship data from scholarshipscorner.website for different regions and combines the data into a single CSV file. The script uses Selenium for web scraping and BeautifulSoup for HTML parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887a0118",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c394d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a9598e",
   "metadata": {},
   "source": [
    "## 2. Define Helper Functions\n",
    "\n",
    "These functions help in extracting specific sections of content from the web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78befcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_section(soup, keywords):\n",
    "    \"\"\"Extract text that comes after a heading containing any of the keywords.\"\"\"\n",
    "    for tag in soup.find_all([\"h2\", \"h3\", \"strong\", \"b\"]):\n",
    "        if any(word in tag.get_text(strip=True).lower() for word in keywords):\n",
    "            content = []\n",
    "            next_tag = tag.find_next_sibling()\n",
    "            while next_tag and next_tag.name in [\"p\", \"ul\"]:\n",
    "                content.append(next_tag.get_text(strip=True))\n",
    "                next_tag = next_tag.find_next_sibling()\n",
    "            return \"\\n\".join(content)\n",
    "    return \"\"\n",
    "\n",
    "def extract_description(content_div):\n",
    "    \"\"\"Extracts main description before any of the key headings.\"\"\"\n",
    "    description = []\n",
    "    for tag in content_div.find_all(recursive=False):\n",
    "        if tag.name in [\"h2\", \"h3\", \"strong\", \"b\"]:\n",
    "            break  # Stop at the first heading\n",
    "        if tag.name in [\"p\", \"ul\"]:\n",
    "            description.append(tag.get_text(strip=True))\n",
    "    return \"\\n\".join(description)\n",
    "\n",
    "def scrape_scholarships(url, region_name):\n",
    "    \"\"\"Scrape scholarships for a specific region.\"\"\"\n",
    "    print(f\"\\n=== Scraping {region_name} scholarships ===\")\n",
    "    \n",
    "    # Setup CSV\n",
    "    csv_filename = f\"scholarships-{region_name.lower().replace(' ', '-')}.csv\"\n",
    "    csv_file = open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\")\n",
    "    csv_writer = csv.DictWriter(csv_file, fieldnames=[\n",
    "        \"Title\", \"Description\", \"Link\", \"Official Link\", \"Deadline\", \"Eligibility\",\n",
    "        \"Host Country\", \"Host University\", \"Program Duration\", \"Degree Offered\", \"Region\", \"Post_at\"\n",
    "    ])\n",
    "    csv_writer.writeheader()\n",
    "    \n",
    "    try:\n",
    "        # Load page\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Get links\n",
    "        post_elements = driver.find_elements(By.CSS_SELECTOR, \"h2.entry-title a\")\n",
    "        post_links = [elem.get_attribute(\"href\") for elem in post_elements]\n",
    "        print(f\"Found {len(post_links)} posts in {region_name}.\")\n",
    "        \n",
    "        # Loop through each post\n",
    "        for index, link in enumerate(post_links):\n",
    "            try:\n",
    "                driver.get(link)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                \n",
    "                # Title\n",
    "                title_element = soup.find(\"h1\", class_=\"entry-title\")\n",
    "                if not title_element:\n",
    "                    print(f\"‚ö†Ô∏è No title found for post {index+1}. Skipping...\")\n",
    "                    continue\n",
    "                    \n",
    "                title = title_element.text.strip()\n",
    "                \n",
    "                # Content area\n",
    "                content_div = soup.find(\"div\", class_=\"entry-content\")\n",
    "                if not content_div:\n",
    "                    print(f\"‚ö†Ô∏è No content found for post: {title}. Skipping...\")\n",
    "                    continue\n",
    "                \n",
    "                # Official Link\n",
    "                official_link = \"\"\n",
    "                for a in content_div.find_all(\"a\"):\n",
    "                    if not a.text:\n",
    "                        continue\n",
    "                    if \"official\" in a.text.lower() or \"apply\" in a.text.lower():\n",
    "                        official_link = a.get(\"href\")\n",
    "                        break\n",
    "                \n",
    "                # Extract sections\n",
    "                deadline = extract_section(content_div, [\"deadline\", \"last date\", \"closing date\"])\n",
    "                description = extract_description(content_div)\n",
    "                eligibility = extract_section(content_div, [\"eligibility\", \"who can apply\", \"eligible\"])\n",
    "                host_country = extract_section(content_div, [\"host country\", \"study in\", \"country\"])\n",
    "                host_university = extract_section(content_div, [\"host university\", \"offered by\", \"university\"])\n",
    "                program_duration = extract_section(content_div, [\"program duration\", \"duration\"])\n",
    "                degree_offered = extract_section(content_div, [\"degree\", \"degree offered\", \"field of study\", \"what you will study\"])\n",
    "                \n",
    "                # Extract post date\n",
    "                post_at = \"\"\n",
    "                post_at_element = soup.select_one(\".entry-date.published\")\n",
    "                if post_at_element:\n",
    "                    post_at = post_at_element.get_text(strip=True)\n",
    "                \n",
    "                # Save\n",
    "                csv_writer.writerow({\n",
    "                    \"Title\": title,\n",
    "                    \"Description\": description,\n",
    "                    \"Link\": link,\n",
    "                    \"Official Link\": official_link,\n",
    "                    \"Deadline\": deadline,\n",
    "                    \"Eligibility\": eligibility,\n",
    "                    \"Host Country\": host_country,\n",
    "                    \"Host University\": host_university,\n",
    "                    \"Program Duration\": program_duration,\n",
    "                    \"Degree Offered\": degree_offered,\n",
    "                    \"Region\": region_name,\n",
    "                    \"Post_at\": post_at\n",
    "                })\n",
    "                \n",
    "                print(f\"‚úÖ {region_name} - Saved {index+1}/{len(post_links)}: {title[:50]}...\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error processing post {index+1} in {region_name} region: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return csv_filename\n",
    "    \n",
    "    finally:\n",
    "        csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d44f60",
   "metadata": {},
   "source": [
    "## 3. Setup Browser and Define Regions\n",
    "\n",
    "Initialize the Chrome WebDriver with appropriate options and define the regions to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cadb11e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup browser\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Define regions and their URLs\n",
    "regions = {\n",
    "    \"Africa\": \"https://scholarshipscorner.website/scholarships-in-africa/\",\n",
    "    \"Asia\": \"https://scholarshipscorner.website/scholarships-in-asia/\",\n",
    "    \"Australia\": \"https://scholarshipscorner.website/scholarships-in-australia/\",\n",
    "    \"Europe\": \"https://scholarshipscorner.website/scholarships-in-europe/\",\n",
    "    \"Middle East\": \"https://scholarshipscorner.website/scholarships-in-middle-east/\",\n",
    "    \"North America\": \"https://scholarshipscorner.website/scholarships-in-north-america/\",\n",
    "    \"South America\": \"https://scholarshipscorner.website/scholarships-in-south-america/\",\n",
    "    \"USA\": \"https://scholarshipscorner.website/scholarships-in-usa/\"\n",
    "}\n",
    "\n",
    "# Create output directory for all CSVs\n",
    "output_dir = \"scholarship_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Store all CSV filenames\n",
    "all_csv_files = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e8163b",
   "metadata": {},
   "source": [
    "## 4. Scrape Scholarships for Each Region\n",
    "\n",
    "Iterate through each region and scrape the scholarship data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558381dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Process each region\n",
    "    for region_name, url in regions.items():\n",
    "        csv_filename = scrape_scholarships(url, region_name)\n",
    "        all_csv_files.append(csv_filename)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in scraping process: {str(e)}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb00ded",
   "metadata": {},
   "source": [
    "## 5. Combine All CSV Files\n",
    "\n",
    "Combine all individual region CSV files into a single master file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e92d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Combine all CSV files into one master file\n",
    "    with open(os.path.join(output_dir, \"all_scholarships.csv\"), \"w\", newline=\"\", encoding=\"utf-8\") as master_file:\n",
    "        # Get fieldnames from the first CSV file\n",
    "        with open(all_csv_files[0], \"r\", encoding=\"utf-8\") as first_file:\n",
    "            reader = csv.reader(first_file)\n",
    "            fieldnames = next(reader)  # First row contains headers\n",
    "        \n",
    "        # Create writer for master file\n",
    "        master_writer = csv.DictWriter(master_file, fieldnames=fieldnames)\n",
    "        master_writer.writeheader()\n",
    "        \n",
    "        # Copy data from each region file\n",
    "        for csv_file in all_csv_files:\n",
    "            with open(csv_file, \"r\", encoding=\"utf-8\") as file:\n",
    "                reader = csv.DictReader(file)\n",
    "                for row in reader:\n",
    "                    master_writer.writerow(row)\n",
    "            \n",
    "            # Move region file to output directory\n",
    "            os.rename(csv_file, os.path.join(output_dir, csv_file))\n",
    "    \n",
    "    print(f\"\\nüéâ All data successfully scraped and combined!\")\n",
    "    print(f\"üìÇ Individual region files and combined data saved in '{output_dir}' folder\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in combining files: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
